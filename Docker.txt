Virtualization vs Containerization:--
    * Traditional way to run a bussiness with servers:--
        * Apps run on servers.
        * A company's email service / DB service / Website used to run on one server each.
        * The OS on those servers didn't have the capability to run multiple apps securely on a single server.
        * Running one application per server was a waste of money.
        * To solve this virtual machines were invented.
    * Virtual Machines :--
        * When it comes to VMs, if you want to run a email, web service, and app on a single server, you install 3
        VMs inside a server.
        * These 3 VMs share the hardware resources of the server
        * VM :-- 
                    server/hardware ---> Hypervisor ---> Virtual machines--> OS--> Apps
                * At bottom server/hardware.
                * Hypervisor is what allows one machine to run multiple virtual machines.
                * Hypervisor allocates and controls the sharing the machines' hardware.
                        Ex :-- VMware ESXi
                           Citrix XenServer
                           Microsoft Hyperv
                * VM uses the shares hardware resources to run the os and app upon it.
                * Each VM has their own OS.
                * Dis-Advantages :-- 
                    * Consume a lot of disk space.
                    * Consumes a lot of RAM and CPU power from server.
                    * Slow to startup.
                    * Requires a license for each OS.

    * Containers:--
                  server/hardware --> OS --> Container Engine --> Apps
         * A container is an app that's been packaged with all the:
                --> Files , Configurations, Dependencies necessary to run it.

         * Disadvantages :--
                --> If the server crashes all the containers will be crashed
                --> A container designed for an OS can't run on other OS
        
    # Containers and VMS can work together.
        --> Ex :-- 2 or more VMS with One or More than One Containers can run.

Docker :--
    * Docker is a software platform that allows you to build, test, and deploy apps quickly using containers.
    * Docker Image :--
    	* A Docker image is a file that contains the instructions and files needed to create a Docker container.
    	* A Docker image acts as a template/blueprint with instructions for building Docker container.
    	* A Docker image stores :-- 
    		* Run time env [ex:-- specific node version]
    		* Application code
    		* Dependencies 
    		* Extra Config [ex :-- .env files ]
    		* Commands.
    		* A file system of it's own
        * Docker images are read-only.
        * Docker container is a runnable instance of Docker image.
        * Containers are Isolated Processes.
        * Meaning, they run independently of any other process running on the computer.
        * Docker images are made up of different layers:--
        	* parent image--> source code --> dependencies --> run commands
        * To create your own image with all the above layers. That docker file will have all the commands & instructions to 
          create a docker image.
        * Refer docker file in the project you did to see how a docker file works.
        * Running docker files, Building Images from them, running containers at a certain port can be done on docker-desktop.
        * When it comes to using terminal commands :--
        	$ docker images ---> Provides a list of images installed
        	$ docker run --name <custom_container_name> <image_name> --> you can't access this cuz, like in docker-desktop's 'optional settings', we didn't specify localhost.
        	$ docker run --name <custom_container_name> -p <port_of_choice>:<port exposed to container> <image_name> 
        	$ docker run --name <custom_container_name> -p <port_of_choice>:<port exorted to container> --rm <image_name> --> This will do the same thing as above, but will delete
        															  container as soom as you stop it.
        	# the above commands will run the containers and you can see that in the terminal. At this point you can't do any thing in the terminal.
        	# to detach the container process from the terminal, so that you can resume working on terminal, use the "-d" flag before <image_name> in the above commands.
        		Ex :-- docker run --name <cusetom_container_name> -p <port_of_choice>:<port_exposed_to_container> -d <image_name>
        	# Docker run runs an image to start a container. If you want to start an existing container with given settings and specified port, in detached mode :--
        		$ docker start <container_name>
        	$ docker ps --> Provides a list of docker container processes running in background.
        	$ docker ps -a --> Provides a list of docker containers even if they aren't running.
        	$ docker stop <container_name> --> This will stop the container.
        * Docker Layer Caching (DLC) :-- 
        	* A feature that speeds up the process of building Docker images by re-using layers that has already been created.
        		* Ex :-- if a docker file has 6 layers:--
        				1. FROM node:current-alpine
        				2. WORKDIR /app
        				3. COPY . .
        				4. RUN npm install
        				5. EXPOSE 4000
        				6. CMD ["node","app.js"]
        			 let's say you made changes to app.js:-
        			 	Now, till layer 3. all layers will be reused from the cache.
        			 	But since you made changes to app.js and it should be COPYied again,
        			 	all the layers from there will be created again. 
         
	* Docker : Managing images & containers :--
		* Deleting docker images :--
			$ docker image rm <image_name> --> Used to delete images only when the containers using it is deleted.
			$ docker image rm <image_name> -f --> Used to delete images even if the containers are using it.
			$ docker iamge rm node:latest --> will delete node:latest
			$ docker image rm node ---------> will delete all node images irrespective of the tags attached.
			
		* Deleting docker containers :--
			$ docker container rm <container_name>
		
		* Deleting all Images & Containers :--
			$ docker system prune -a 
		
        * Docker Volumes :--
        	* Usually when you make changes to project code, you have the rebuild the docker image with the dockerfile and then start the container to see changes.
        	There's way to solve this by using volumes:--
        		* Volumes are a docker feature that allows data to be stored and managed persistently outside containers.
        		* We are going to map the code folder to the volume folder so that i can keep track of the changes.
        		* To Create a Volume :--
        			$ docker run --name <container_name> -p <port_of_ur_choice> : <exposed_port_for_container> --rm -v <absolute_path_of_Code_folder[not relative]> : <working_dir> <image_name>
        			* The Above command will map all the code files to a volume folder inside the container. So any changes in the code folder will sync with the same files inside container.
        			 But the problem is, if we delete node_modules folder [which we don't need], the same folder in container will also be deleted which we don't want to happen.
        			* So we have to find a way to sync all the changes in files in original folder with volume in the container. But, keep "node_modules" folder uneffected. so do the following:--
        			$ docker run --name <container_name> -p <port_of_ur_choice> : <exporsed_port> --rm -v <abs_path_of_code> : <working_dir> -v /app/node_modules <image_name>
        			* If you observe properly, '/app/node_modules' folder is not the original code folder but the mapped folder inside the container. This will again in turn be mapped to some
        			unknown different volume folder managed by docker. This overrides the provious volume folder and maps only the 'node_modules' folder to a different volume.
        			* NOW IF YOU MAKE ANY KIND OF CHANGES TO CODE, IT WILL REFLECT. NO NEED TO DELETE AND CREATE IMAGE AGAIN WITH THE DOCKER FILE.

	* Docker Compose :--
		* 
